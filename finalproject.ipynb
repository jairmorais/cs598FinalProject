{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Final Project Draft - CS598 DL4H in Spring 2023** \n",
        "\n",
        "  Jair Morais - jairv2@illinois.edu\n",
        "\n",
        "\n",
        "Group ID: 210\n",
        "\n",
        "Paper ID: 24"
      ],
      "metadata": {
        "id": "1Y62jf66W4-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install stop_words module:"
      ],
      "metadata": {
        "id": "Q-K9W4zXX4-d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVqAX9Rrg2Ev",
        "outputId": "1e736592-e34d-45c3-edda-a1028f729089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stop_words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop_words\n",
            "  Building wheel for stop_words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32910 sha256=498262a88593b3c6644c9a0d1b75f68e22162ef83b31723a516825f1ceb794af\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
            "Successfully built stop_words\n",
            "Installing collected packages: stop_words\n",
            "Successfully installed stop_words-2018.7.23\n"
          ]
        }
      ],
      "source": [
        "!pip install stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Declaration:"
      ],
      "metadata": {
        "id": "X9cJUsk-YCRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_2-lqqtg2E3"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "import string\n",
        "import numpy as np\n",
        "from stop_words import get_stop_words    # download stop words package from https://pypi.org/project/stop-words/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is necessary to put the noteevents.csv and diagnoses_icd.csv on the root path to run the code.\n",
        "\n",
        "The Noteevent table at MIMIC-III (this table re-\n",
        "quire credential access to the physionet database),\n",
        "the diagnosis icd table at MIMIC-III, it will be used\n",
        "the demo version of that."
      ],
      "metadata": {
        "id": "LoGfbedtZjN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofx9o3DBg2E5"
      },
      "outputs": [],
      "source": [
        "stop_words = get_stop_words('english')\n",
        "admidic=defaultdict(list)\n",
        "count=0\n",
        "\n",
        "\n",
        "with open('NOTEEVENTS2.csv', 'r') as csvfile:\n",
        "     spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
        "     for row in spamreader:\n",
        "         dischargesummary = row[6].strip()\n",
        "        \n",
        "         if dischargesummary in \"Discharge summary\":\n",
        "             admidic[row[2]].append(row[-1].replace('\\n',' ').translate(str.maketrans('','',string.punctuation)).lower())\n",
        "             count=count+1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGxV2urUg2E7"
      },
      "outputs": [],
      "source": [
        "u=defaultdict(int)\n",
        "for i in admidic:\n",
        "    for jj in admidic[i]:\n",
        "        line=jj.strip('\\n').split()\n",
        "        for j in line:\n",
        "            u[j]=u[j]+1\n",
        "\n",
        "u2=defaultdict(int)\n",
        "for i in u:\n",
        "        if i.isdigit()==False:\n",
        "            if u[i]>10:\n",
        "                if i not in stop_words:\n",
        "                    u2[i]=u[i]\n",
        "                    \n",
        "u=[]   \n",
        "\n",
        "file1=codecs.open('DIAGNOSES_ICD2.csv','r')\n",
        "ad2c=defaultdict(list)\n",
        "line=file1.readline()\n",
        "\n",
        "while line:\n",
        "    line=line.strip().split(',')\n",
        "\n",
        "    if line[4][1:-1]!='':\n",
        "        ad2c[line[2]].append(\"d_\"+line[4][1:-1])\n",
        "    \n",
        "    line=file1.readline()\n",
        "\n",
        "codeu=defaultdict(int)\n",
        "for i in ad2c:\n",
        "    for j in ad2c[i]:\n",
        "        codeu[j]=codeu[j]+1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI7OhQQ_g2E9"
      },
      "outputs": [],
      "source": [
        "cthre=0\n",
        "fileo=codecs.open(\"combined_dataset\",'w')\n",
        "\n",
        "IDlist=np.load('IDlist.npy',encoding='bytes').astype(str)\n",
        "\n",
        "for i in IDlist:\n",
        "    if ad2c[i]!=[]:\n",
        "        fileo.write('start! '+i+'\\n')\n",
        "        fileo.write('codes: ')\n",
        "        tempc=[]\n",
        "        for code in ad2c[i]:\n",
        "            if codeu[code]>=cthre:\n",
        "                if code[0:5] not in tempc:\n",
        "                    tempc.append(code[0:5])\n",
        "       \n",
        "        for code in tempc:\n",
        "            fileo.write(code+\" \")\n",
        "        fileo.write('\\n')\n",
        "        fileo.write('notes:\\n')\n",
        "      \n",
        "        for line in admidic[i]:    \n",
        "            thisline=line.strip('\\n').split() \n",
        "            \n",
        "            for j in thisline:\n",
        "                \n",
        "                if u2[j]!=0:\n",
        "                    \n",
        "                    fileo.write(j+\" \")\n",
        "            fileo.write('\\n')\n",
        "        fileo.write('end!\\n')\n",
        "fileo.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIKQJ4REg2E-"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "wikivocab={}\n",
        "file1=codecs.open(\"wikipedia_knowledge\",'r','utf-8')\n",
        "line=file1.readline()\n",
        "while line:\n",
        "    if line[0:3]!='XXX':\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        for i in line:\n",
        "            wikivocab[i.lower()]=1\n",
        "    line=file1.readline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX8xS9wEg2E_"
      },
      "outputs": [],
      "source": [
        "notesvocab={}\n",
        "filec=codecs.open(\"combined_dataset\",'r','utf-8')\n",
        "\n",
        "line=filec.readline()\n",
        "\n",
        "while line:\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    \n",
        "    if line[0]=='codes:':\n",
        "        line=filec.readline()\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        \n",
        "        if  line[0]=='notes:':\n",
        "            \n",
        "            line=filec.readline()\n",
        "            \n",
        "            while line!='end!\\n':\n",
        "                line=line.strip('\\n')\n",
        "                line=line.split()\n",
        "                for word in line:\n",
        "                    notesvocab[word]=1\n",
        "                \n",
        "                line=filec.readline()\n",
        "                \n",
        "            \n",
        "    line=filec.readline()\n",
        "\n",
        "\n",
        "\n",
        "a1=set(notesvocab)\n",
        "a2=set(wikivocab)\n",
        "a3=a1.intersection(a2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7cFSYP7g2FA"
      },
      "outputs": [],
      "source": [
        "wikidocuments=[]\n",
        "file2=codecs.open(\"wikipedia_knowledge\",'r','utf-8')\n",
        "line=file2.readline()\n",
        "while line:\n",
        "    if line[0:4]=='XXXd':\n",
        "        tempf=[]\n",
        "        line=file2.readline()\n",
        "        while line[0:4]!='XXXe':\n",
        "            line=line.strip('\\n')\n",
        "            line=line.split()\n",
        "            for i in line:\n",
        "                if i.lower() in a3:\n",
        "                    tempf.append(i.lower())\n",
        "            line=file2.readline()\n",
        "        wikidocuments.append(tempf)\n",
        "        \n",
        "    line=file2.readline()\n",
        "\n",
        "\n",
        "notesdocuments=[]\n",
        "file3=codecs.open(\"combined_dataset\",'r','utf-8')\n",
        "\n",
        "line=file3.readline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpTC1R3Yg2FB"
      },
      "outputs": [],
      "source": [
        "while line:\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    if line[0]=='codes:':\n",
        "        line=file3.readline()\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        \n",
        "        if  line[0]=='notes:':\n",
        "            tempf=[]\n",
        "            line=file3.readline()\n",
        "        \n",
        "            while line!='end!\\n':\n",
        "                line=line.strip('\\n')\n",
        "                line=line.split()\n",
        "                for word in line:\n",
        "                    if word in a3:\n",
        "                        tempf.append(word)\n",
        "                \n",
        "                line=file3.readline()\n",
        "                \n",
        "            \n",
        "            notesdocuments.append(tempf)\n",
        "    line=file3.readline()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTu4ljcrg2FB"
      },
      "outputs": [],
      "source": [
        "\n",
        "notesvocab={}\n",
        "for i in notesdocuments:\n",
        "    for j in i:\n",
        "        if j.lower() not in notesvocab:\n",
        "            notesvocab[j.lower()]=len(notesvocab)\n",
        "notedata=[]\n",
        "for i in notesdocuments:\n",
        "    temp=''\n",
        "    for j in i:\n",
        "        temp=temp+j+\" \"\n",
        "    notedata.append(temp)\n",
        "    \n",
        "\n",
        "wikidata=[]\n",
        "for i in wikidocuments:\n",
        "    temp=''\n",
        "    for j in i:\n",
        "        temp=temp+j+\" \"\n",
        "    wikidata.append(temp)    \n",
        "##########################################################\n",
        "vect = CountVectorizer(min_df=1,vocabulary=notesvocab,binary=True)\n",
        "binaryn = vect.fit_transform(notedata)\n",
        "binaryn=binaryn.A\n",
        "binaryn=np.array(binaryn,dtype=float)\n",
        "\n",
        "vect2 = CountVectorizer(min_df=1,vocabulary=notesvocab,binary=True)\n",
        "binaryk = vect2.fit_transform(wikidata)\n",
        "binaryk=binaryk.A\n",
        "binaryk=np.array(binaryk,dtype=float)\n",
        "\n",
        "\n",
        "np.save('notevec',binaryn)\n",
        "np.save('wikivec',binaryk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQno3rAGg2FE",
        "outputId": "f4006e7b-eb2f-478d-a8fb-bab0ace9595c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1f/ddtsmdgd43dby3cx3r_v_xpr0000gn/T/ipykernel_67828/94965002.py:98: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  data=np.array(data)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import codecs\n",
        "\n",
        "\n",
        "##########################################################\n",
        "wikivoc={}\n",
        "codewiki=defaultdict(list)\n",
        "\n",
        "file2=codecs.open(\"wikipedia_knowledge\",'r','utf-8')\n",
        "line=file2.readline()\n",
        "count=0\n",
        "while line:\n",
        "    if line[0:4]=='XXXd':\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        for i in line:\n",
        "            if i[0:2]=='d_':\n",
        "                codewiki[i].append(count)\n",
        "                wikivoc[i]=1\n",
        "        count=count+1\n",
        "    line=file2.readline()\n",
        "\n",
        "################### four codes have two wikidocuments, correct them\n",
        "codewiki['d_072']=[214]\n",
        "codewiki['d_698']=[125]\n",
        "codewiki['d_305']=[250]\n",
        "codewiki['d_386']=[219]\n",
        "\n",
        "np.save('wikivoc',wikivoc)\n",
        "##################################################\n",
        "filec=codecs.open(\"combined_dataset\",'r','utf-8')\n",
        "\n",
        "line=filec.readline()\n",
        "\n",
        "feature=[]\n",
        "label=[]\n",
        "\n",
        "while line:\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    \n",
        "    if line[0]=='codes:':\n",
        "        temp=line[1:]\n",
        "        label.append(temp)\n",
        "        line=filec.readline()\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        if  line[0]=='notes:':\n",
        "            tempf=[]\n",
        "            line=filec.readline()\n",
        "           \n",
        "            while line!='end!\\n':\n",
        "                line=line.strip('\\n')\n",
        "                line=line.split()\n",
        "                tempf=tempf+line\n",
        "                line=filec.readline()\n",
        "            feature.append(tempf)\n",
        "    line=filec.readline()\n",
        "\n",
        "\n",
        "prevoc={}\n",
        "for i in label:\n",
        "    for j in i:\n",
        "        if j not in prevoc:\n",
        "            prevoc[j]=len(prevoc)\n",
        "\n",
        "##################################\n",
        "notevec=np.load('notevec.npy')\n",
        "wikivec=np.load('wikivec.npy')\n",
        "label_to_ix = {}\n",
        "ix_to_label={}\n",
        "\n",
        "\n",
        "for codes in label:\n",
        "    for code in codes:\n",
        "        if code not in label_to_ix:\n",
        "            label_to_ix[code]=len(label_to_ix)\n",
        "            ix_to_label[label_to_ix[code]]=code\n",
        "\n",
        "tempwikivec=[]\n",
        "\n",
        "for i in range(0,len(ix_to_label)):\n",
        "    if ix_to_label[i] in wikivoc:\n",
        "        temp=wikivec[codewiki[ix_to_label[i]][0]]\n",
        "        tempwikivec.append(temp)\n",
        "    else:\n",
        "        tempwikivec.append([0.0]*wikivec.shape[1])\n",
        "wikivec=np.array(tempwikivec)\n",
        "\n",
        "####################################\n",
        "\n",
        "data=[]\n",
        "for i in range(0,len(feature)):\n",
        "    data.append((feature[i], notevec[i], label[i]))\n",
        "    \n",
        "data=np.array(data)  \n",
        "\n",
        "label_to_ix = {}\n",
        "ix_to_label={}\n",
        "\n",
        "for doc, note, codes in data:\n",
        "    for code in codes:\n",
        "        if code not in label_to_ix:\n",
        "            if code in wikivoc:\n",
        "                label_to_ix[code]=len(label_to_ix)\n",
        "                ix_to_label[label_to_ix[code]]=code\n",
        "\n",
        "np.save('label_to_ix',label_to_ix)\n",
        "np.save('ix_to_label',ix_to_label)\n",
        "\n",
        "training_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "training_data, val_data = train_test_split(training_data, test_size=0.125, random_state=42)\n",
        "\n",
        "np.save('training_data',training_data)\n",
        "np.save('test_data',test_data)\n",
        "np.save('val_data',val_data)\n",
        "\n",
        "\n",
        "word_to_ix = {}\n",
        "ix_to_word={}\n",
        "ix_to_word[0]='OUT'\n",
        "\n",
        "\n",
        "for doc, note, codes in training_data:\n",
        "    for word in doc:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)+1\n",
        "            ix_to_word[word_to_ix[word]]=word  \n",
        "    \n",
        "np.save('word_to_ix',word_to_ix)\n",
        "np.save('ix_to_word',ix_to_word)\n",
        "\n",
        "newwikivec=[]\n",
        "for i in range(0,len(ix_to_label)):\n",
        "    newwikivec.append(wikivec[prevoc[ix_to_label[i]]])\n",
        "newwikivec=np.array(newwikivec)\n",
        "np.save('newwikivec',newwikivec)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KSI_CAML**"
      ],
      "metadata": {
        "id": "5kuiwX-4aYRk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "sNifRniPg2FF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "\n",
        "##########################################################\n",
        "\n",
        "label_to_ix=np.load('label_to_ix.npy',allow_pickle=True).item()\n",
        "ix_to_label=np.load('ix_to_label.npy',allow_pickle=True)\n",
        "training_data=np.load('training_data.npy',allow_pickle=True)\n",
        "test_data=np.load('test_data.npy',allow_pickle=True)\n",
        "val_data=np.load('val_data.npy',allow_pickle=True)\n",
        "word_to_ix=np.load('word_to_ix.npy',allow_pickle=True).item()\n",
        "ix_to_word=np.load('ix_to_word.npy',allow_pickle=True)\n",
        "newwikivec=np.load('newwikivec.npy',allow_pickle=True)\n",
        "wikivoc=np.load('wikivoc.npy',allow_pickle=True).item()\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "\n",
        "batchsize=8\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYueZsAcg2FG"
      },
      "outputs": [],
      "source": [
        "def preprocessing(data):\n",
        "    new_data=[]\n",
        "    for i, note, j in data:\n",
        "        templabel=[0.0]*len(label_to_ix)\n",
        "        for jj in j:\n",
        "            if jj in wikivoc:\n",
        "                templabel[label_to_ix[jj]]=1.0\n",
        "        templabel=np.array(templabel,dtype=float)\n",
        "        new_data.append((i, note, templabel))\n",
        "    #new_data=np.array(new_data)\n",
        "    \n",
        "    lenlist=[]\n",
        "    for i in new_data:\n",
        "        lenlist.append(len(i[0]))\n",
        "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
        "    \n",
        "    data_transform = []\n",
        "    for j in sortlen:\n",
        "        data_transform.append(new_data[j])\n",
        "    new_data=data_transform #new_data[sortlen]\n",
        "    \n",
        "    batch_data=[]\n",
        "    \n",
        "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
        "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
        "        mybsize= len(thisblock)\n",
        "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
        "        main_matrix = np.zeros((mybsize, numword), dtype= np.int32)\n",
        "        for i in range(main_matrix.shape[0]):\n",
        "            for j in range(main_matrix.shape[1]):\n",
        "                try:\n",
        "                    if thisblock[i][0][j] in word_to_ix:\n",
        "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
        "                    \n",
        "                except IndexError:\n",
        "                    pass       # because initialze with 0, so you pad with 0\n",
        "    \n",
        "        xxx2=[]\n",
        "        yyy=[]\n",
        "        for ii in thisblock:\n",
        "            xxx2.append(ii[1])\n",
        "            yyy.append(ii[2])\n",
        "        \n",
        "        xxx2=np.array(xxx2)\n",
        "        yyy=np.array(yyy)\n",
        "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
        "    return batch_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "mPsDJ9vSg2FH"
      },
      "outputs": [],
      "source": [
        "batchtraining_data=preprocessing(training_data)\n",
        "batchtest_data=preprocessing(test_data)\n",
        "batchval_data=preprocessing(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "bQqFuACMg2FI",
        "outputId": "c3c26ad8-46b9-4d54-892d-df94b0557e5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_training\n",
            "0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.2111111111111111\n",
            "1\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "2\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "3\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "4\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "5\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "6\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "[0.2111111111111111, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556] 1\n",
            "start_training\n",
            "0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "1\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "2\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "3\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "4\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "5\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5888888888888889\n",
            "6\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.4222222222222222\n",
            "[0.5555555555555556, 0.611111111111111, 0.611111111111111, 0.611111111111111, 0.611111111111111, 0.5888888888888889, 0.4222222222222222] 1\n",
            "CAML alone:           \n",
            "test loss 0.14308937142292658\n",
            "top- 10 0.6630434782608695\n",
            "macro AUC 0.47779546150337254\n",
            "micro AUC 0.6965360856986473\n",
            "macro F1 0.0\n",
            "micro F1 0.0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "KSI+CAML:           \n",
            "test loss 0.14226043969392776\n",
            "top- 10 0.6543478260869565\n",
            "macro AUC 0.5388739760460503\n",
            "micro AUC 0.7243725076237392\n",
            "macro F1 0.0\n",
            "micro F1 0.0\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize=100\n",
        "hidden_dim=200\n",
        "class CAML(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(CAML, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.embed_drop = nn.Dropout(p=0.2)   \n",
        "        \n",
        "        \n",
        "        self.convs1 = nn.Conv1d(Embeddingsize,300,10,padding=5)\n",
        "        self.H=nn.Linear(300, tagset_size )   \n",
        "        self.final = nn.Linear(300, tagset_size)\n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize,bias=False)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "    \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "        \n",
        "       \n",
        "        thisembeddings=self.word_embeddings(vec1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "        thisembeddings=thisembeddings.transpose(1,2)\n",
        "        \n",
        "        \n",
        "        thisembeddings=self.tanh(self.convs1(thisembeddings).transpose(1,2))  \n",
        "        \n",
        "        alpha=self.H.weight.matmul(thisembeddings.transpose(1,2))\n",
        "        alpha=F.softmax(alpha, dim=2)\n",
        "        \n",
        "        m=alpha.matmul(thisembeddings)\n",
        "       \n",
        "        myfinal=self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "        \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "       \n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(myfinal.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(myfinal)\n",
        "        \n",
        "        \n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "\n",
        "topk=10\n",
        "\n",
        "def trainmodel(model, sim):\n",
        "    print ('start_training')\n",
        "    modelsaved=[]\n",
        "    modelperform=[]\n",
        "    topk=10\n",
        "    \n",
        "    \n",
        "    bestresults=-1\n",
        "    bestiter=-1\n",
        "    for epoch in range(5000):  \n",
        "        model.train()\n",
        "        \n",
        "        lossestrain = []\n",
        "        recall=[]\n",
        "        for mysentence in batchtraining_data:\n",
        "            model.zero_grad()\n",
        "            \n",
        "            targets = mysentence[2].cuda()\n",
        "            tag_scores = model(mysentence[0].cuda(),mysentence[1].cuda(),wikivec.cuda(),sim)\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossestrain.append(loss.data.mean())\n",
        "        print (epoch)\n",
        "        modelsaved.append(copy.deepcopy(model.state_dict()))\n",
        "        print (\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
        "        model.eval()\n",
        "    \n",
        "        recall=[]\n",
        "        for inputs in batchval_data:\n",
        "           \n",
        "            targets = inputs[2].cuda()\n",
        "            tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "    \n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            \n",
        "            targets=targets.data.cpu().numpy()\n",
        "            tag_scores= tag_scores.data.cpu().numpy()\n",
        "            \n",
        "            \n",
        "            for iii in range(0,len(tag_scores)):\n",
        "                temp={}\n",
        "                for iiii in range(0,len(tag_scores[iii])):\n",
        "                    temp[iiii]=tag_scores[iii][iiii]\n",
        "                temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "                thistop=int(np.sum(targets[iii]))\n",
        "                hit=0.0\n",
        "                for ii in temp1[0:max(thistop,topk)]:\n",
        "                    if targets[iii][ii[0]]==1.0:\n",
        "                        hit=hit+1\n",
        "                if thistop!=0:\n",
        "                    recall.append(hit/thistop)\n",
        "            \n",
        "        print ('validation top-',topk, np.mean(recall))\n",
        "        \n",
        "        \n",
        "        \n",
        "        modelperform.append(np.mean(recall))\n",
        "        if modelperform[-1]>bestresults:\n",
        "            bestresults=modelperform[-1]\n",
        "            bestiter=len(modelperform)-1\n",
        "        \n",
        "        if(len(modelperform)-bestiter)>5:\n",
        "            print (modelperform,bestiter)\n",
        "            return modelsaved[bestiter]\n",
        "    \n",
        "model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "basemodel= trainmodel(model, 0)\n",
        "torch.save(basemodel, 'CAML_model')\n",
        "\n",
        "model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "model.load_state_dict(basemodel)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "KSImodel= trainmodel(model, 1)\n",
        "torch.save(KSImodel, 'KSI_CAML_model')\n",
        "\n",
        "def testmodel(modelstate, sim):\n",
        "    model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "    model.cuda()\n",
        "    model.load_state_dict(modelstate)\n",
        "    loss_function = nn.BCELoss()\n",
        "    model.eval()\n",
        "    recall=[]\n",
        "    lossestest = []\n",
        "    \n",
        "    y_true=[]\n",
        "    y_scores=[]\n",
        "    \n",
        "    \n",
        "    for inputs in batchtest_data:\n",
        "       \n",
        "        targets = inputs[2].cuda()\n",
        "        \n",
        "        tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        \n",
        "        targets=targets.data.cpu().numpy()\n",
        "        tag_scores= tag_scores.data.cpu().numpy()\n",
        "        \n",
        "        \n",
        "        lossestest.append(loss.data.mean())\n",
        "        y_true.append(targets)\n",
        "        y_scores.append(tag_scores)\n",
        "        \n",
        "        for iii in range(0,len(tag_scores)):\n",
        "            temp={}\n",
        "            for iiii in range(0,len(tag_scores[iii])):\n",
        "                temp[iiii]=tag_scores[iii][iiii]\n",
        "            temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "            thistop=int(np.sum(targets[iii]))\n",
        "            hit=0.0\n",
        "            \n",
        "            for ii in temp1[0:max(thistop,topk)]:\n",
        "                if targets[iii][ii[0]]==1.0:\n",
        "                    hit=hit+1\n",
        "            if thistop!=0:\n",
        "                recall.append(hit/thistop)\n",
        "    y_true=np.concatenate(y_true,axis=0)\n",
        "    y_scores=np.concatenate(y_scores,axis=0)\n",
        "    y_true=y_true.T\n",
        "    y_scores=y_scores.T\n",
        "    temptrue=[]\n",
        "    tempscores=[]\n",
        "    for  col in range(0,len(y_true)):\n",
        "        if np.sum(y_true[col])!=0:\n",
        "            temptrue.append(y_true[col])\n",
        "            tempscores.append(y_scores[col])\n",
        "    temptrue=np.array(temptrue)\n",
        "    tempscores=np.array(tempscores)\n",
        "    y_true=temptrue.T\n",
        "    y_scores=tempscores.T\n",
        "    y_pred=(y_scores>0.5).astype(np.int32)\n",
        "    lossarray = []\n",
        "    \n",
        "    for i in lossestest:\n",
        "        lossarray.append(i.item())\n",
        "    print ('test loss', np.mean(lossarray))\n",
        "    print ('top-',topk, np.mean(recall))\n",
        "    print ('macro AUC', roc_auc_score(y_true, y_scores,average='macro'))\n",
        "    print ('micro AUC', roc_auc_score(y_true, y_scores,average='micro'))\n",
        "    print ('macro F1', f1_score(y_true, y_pred, average='macro')  )\n",
        "    print ('micro F1', f1_score(y_true, y_pred, average='micro')  )\n",
        "\n",
        "print ('CAML alone:           ')\n",
        "testmodel(basemodel, 0)\n",
        "print ('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
        "print ('KSI+CAML:           ')\n",
        "testmodel(KSImodel, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KSI_CNN**"
      ],
      "metadata": {
        "id": "vkw0GxdkajwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "\n",
        "##########################################################\n",
        "\n",
        "label_to_ix=np.load('label_to_ix.npy',allow_pickle=True).item()\n",
        "ix_to_label=np.load('ix_to_label.npy',allow_pickle=True)\n",
        "training_data=np.load('training_data.npy',allow_pickle=True)\n",
        "test_data=np.load('test_data.npy',allow_pickle=True)\n",
        "val_data=np.load('val_data.npy',allow_pickle=True)\n",
        "word_to_ix=np.load('word_to_ix.npy',allow_pickle=True).item()\n",
        "ix_to_word=np.load('ix_to_word.npy',allow_pickle=True)\n",
        "newwikivec=np.load('newwikivec.npy',allow_pickle=True)\n",
        "wikivoc=np.load('wikivoc.npy',allow_pickle=True).item()\n",
        "\n",
        "\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "batchsize=8\n",
        "\n",
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    new_data=[]\n",
        "    for i, note, j in data:\n",
        "        templabel=[0.0]*len(label_to_ix)\n",
        "        for jj in j:\n",
        "            if jj in wikivoc:\n",
        "                templabel[label_to_ix[jj]]=1.0\n",
        "        templabel=np.array(templabel,dtype=float)\n",
        "        new_data.append((i, note, templabel))\n",
        "    new_data=np.array(new_data,dtype=object)\n",
        "    \n",
        "    lenlist=[]\n",
        "    for i in new_data:\n",
        "        lenlist.append(len(i[0]))\n",
        "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
        "    new_data=new_data[sortlen]\n",
        "\n",
        "    batch_data=[]\n",
        "    \n",
        "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
        "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
        "        mybsize= len(thisblock)\n",
        "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
        "        main_matrix = np.zeros((mybsize, numword), dtype= np.int32)\n",
        "        for i in range(main_matrix.shape[0]):\n",
        "            for j in range(main_matrix.shape[1]):\n",
        "                try:\n",
        "                    if thisblock[i][0][j] in word_to_ix:\n",
        "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
        "                    \n",
        "                except IndexError:\n",
        "                    pass       # because initialze with 0, so you pad with 0\n",
        "    \n",
        "        xxx2=[]\n",
        "        yyy=[]\n",
        "        for ii in thisblock:\n",
        "            xxx2.append(ii[1])\n",
        "            yyy.append(ii[2])\n",
        "        \n",
        "        xxx2=np.array(xxx2)\n",
        "        yyy=np.array(yyy)\n",
        "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
        "        \n",
        "    return batch_data\n",
        "\n",
        "batchtraining_data=preprocessing(training_data)\n",
        "batchtest_data=preprocessing(test_data)\n",
        "batchval_data=preprocessing(val_data)\n"
      ],
      "metadata": {
        "id": "VGAmYQrk_3m-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize=100\n",
        "hidden_dim=200\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.embed_drop = nn.Dropout(p=0.2)\n",
        "        \n",
        "        self.hidden2tag = nn.Linear(300, tagset_size)\n",
        "        \n",
        "        \n",
        "        self.convs1 = nn.Conv1d(Embeddingsize,100,3)\n",
        "        self.convs2 = nn.Conv1d(Embeddingsize,100,4)\n",
        "        self.convs3 = nn.Conv1d(Embeddingsize,100,5)\n",
        "        \n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1,bias=False)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "       \n",
        "        thisembeddings=self.word_embeddings(vec1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "        thisembeddings=thisembeddings.transpose(1,2)\n",
        "        \n",
        "        output1=self.tanh(self.convs1(thisembeddings))\n",
        "        output1=nn.MaxPool1d(output1.size()[2])(output1)\n",
        "        \n",
        "        output2=self.tanh(self.convs2(thisembeddings))\n",
        "        output2=nn.MaxPool1d(output2.size()[2])(output2)\n",
        "        \n",
        "        output3=self.tanh(self.convs3(thisembeddings))\n",
        "        output3=nn.MaxPool1d(output3.size()[2])(output3)\n",
        "        \n",
        "        output4 = torch.cat([output1,output2,output3], 1).squeeze(2)\n",
        "        \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "       \n",
        "        vec2 = self.hidden2tag(output4)\n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(vec2.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(vec2)\n",
        "        \n",
        "        \n",
        "        return tag_scores\n",
        "\n",
        "######################################################################\n",
        "# Train the model:\n",
        "\n",
        "topk=10\n",
        "\n",
        "def trainmodel(model, sim):\n",
        "    print ('start_training')\n",
        "    modelsaved=[]\n",
        "    modelperform=[]\n",
        "    topk=10\n",
        "    \n",
        "    \n",
        "    bestresults=-1\n",
        "    bestiter=-1\n",
        "    for epoch in range(5000):  \n",
        "        model.train()\n",
        "        \n",
        "        lossestrain = []\n",
        "        recall=[]\n",
        "        for mysentence in batchtraining_data:\n",
        "            model.zero_grad()\n",
        "            \n",
        "            targets = mysentence[2].cuda()\n",
        "            tag_scores = model(mysentence[0].cuda(),mysentence[1].cuda(),wikivec.cuda(),sim)\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossestrain.append(loss.data.mean())\n",
        "        print (epoch)\n",
        "        modelsaved.append(copy.deepcopy(model.state_dict()))\n",
        "        print (\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
        "        model.eval()\n",
        "    \n",
        "        recall=[]\n",
        "        for inputs in batchval_data:\n",
        "           \n",
        "            targets = inputs[2].cuda()\n",
        "            tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "    \n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            \n",
        "            targets=targets.data.cpu().numpy()\n",
        "            tag_scores= tag_scores.data.cpu().numpy()\n",
        "            \n",
        "            \n",
        "            for iii in range(0,len(tag_scores)):\n",
        "                temp={}\n",
        "                for iiii in range(0,len(tag_scores[iii])):\n",
        "                    temp[iiii]=tag_scores[iii][iiii]\n",
        "                temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "                thistop=int(np.sum(targets[iii]))\n",
        "                hit=0.0\n",
        "                for ii in temp1[0:max(thistop,topk)]:\n",
        "                    if targets[iii][ii[0]]==1.0:\n",
        "                        hit=hit+1\n",
        "                if thistop!=0:\n",
        "                    recall.append(hit/thistop)\n",
        "            \n",
        "        print ('validation top-',topk, np.mean(recall))\n",
        "        \n",
        "        \n",
        "        \n",
        "        modelperform.append(np.mean(recall))\n",
        "        if modelperform[-1]>bestresults:\n",
        "            bestresults=modelperform[-1]\n",
        "            bestiter=len(modelperform)-1\n",
        "        \n",
        "        if (len(modelperform)-bestiter)>5:\n",
        "            print (modelperform,bestiter)\n",
        "            return modelsaved[bestiter]\n",
        "    \n",
        "model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "basemodel= trainmodel(model, 0)\n",
        "torch.save(basemodel, 'CNN_model')\n",
        "\n",
        "model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "model.load_state_dict(basemodel)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "KSImodel= trainmodel(model, 1)\n",
        "torch.save(KSImodel, 'KSI_CNN_model')\n",
        "\n",
        "def testmodel(modelstate, sim):\n",
        "    model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "    model.cuda()\n",
        "    model.load_state_dict(modelstate)\n",
        "    loss_function = nn.BCELoss()\n",
        "    model.eval()\n",
        "    recall = []\n",
        "    lossestest = []\n",
        "    \n",
        "    y_true = []  # Initialize as empty list\n",
        "    y_scores = []  # Initialize as empty list\n",
        "   \n",
        "    for inputs in batchtest_data:\n",
        "        \n",
        "        targets = inputs[2].cuda()\n",
        "        tag_scores = model(inputs[0].cuda(), inputs[1].cuda(), wikivec.cuda(), sim)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        \n",
        "        targets = targets.data.cpu().numpy()\n",
        "        tag_scores = tag_scores.data.cpu().numpy()\n",
        "        \n",
        "        lossestest.append(loss.data.mean())\n",
        "        y_true.append(targets)\n",
        "        y_scores.append(tag_scores)\n",
        "        for iii in range(0, len(tag_scores)):\n",
        "            temp = {}\n",
        "            for iiii in range(0, len(tag_scores[iii])):\n",
        "                temp[iiii] = tag_scores[iii][iiii]\n",
        "            temp1 = [(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "            thistop = int(np.sum(targets[iii]))\n",
        "            hit = 0.0\n",
        "            \n",
        "            for ii in temp1[0:max(thistop, topk)]:\n",
        "                if targets[iii][ii[0]] == 1.0:\n",
        "                    hit = hit + 1\n",
        "            if thistop != 0:\n",
        "                recall.append(hit / thistop)\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "    y_scores = np.concatenate(y_scores, axis=0)\n",
        "\n",
        "    y_true = y_true.T\n",
        "    y_scores = y_scores.T\n",
        "    temptrue = []\n",
        "    tempscores = []\n",
        "    for col in range(0, len(y_true)):\n",
        "        if np.sum(y_true[col]) != 0:\n",
        "            temptrue.append(y_true[col])\n",
        "            tempscores.append(y_scores[col])\n",
        "    temptrue = np.array(temptrue)\n",
        "    tempscores = np.array(tempscores)\n",
        "    y_true = temptrue.T\n",
        "    y_scores = tempscores.T\n",
        "    y_pred = (y_scores > 0.5).astype(np.int32)\n",
        "    lossarray = []\n",
        "    \n",
        "    for i in lossestest:\n",
        "        lossarray.append(i.item())\n",
        "        \n",
        "    print('test loss', np.mean(lossarray))\n",
        "    print('top-', topk, np.mean(recall))\n",
        "    print('macro AUC', roc_auc_score(y_true, y_scores, average='macro'))\n",
        "    print('micro AUC', roc_auc_score(y_true, y_scores, average='micro'))\n",
        "    print('macro F1', f1_score(y_true, y_pred, average='macro'))\n",
        "    print('micro F1', f1_score(y_true, y_pred, average='micro'))\n",
        "\n",
        "print ('CNN alone:           ')\n",
        "testmodel(basemodel, 0)\n",
        "print ('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
        "print ('KSI+CNN:           ')\n",
        "testmodel(KSImodel, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LdTZCfUlysS",
        "outputId": "ac10dbf5-6f84-4c84-adb9-48e0bf84f4f0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_training\n",
            "0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.14444444444444446\n",
            "1\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "2\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "3\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "4\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "5\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "6\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "[0.14444444444444446, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556] 1\n",
            "start_training\n",
            "0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "1\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "2\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "3\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "4\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "5\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.6444444444444445\n",
            "6\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5888888888888889\n",
            "7\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.4222222222222222\n",
            "8\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.3888888888888889\n",
            "9\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.35555555555555557\n",
            "10\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5\n",
            "[0.5555555555555556, 0.611111111111111, 0.5555555555555556, 0.611111111111111, 0.611111111111111, 0.6444444444444445, 0.5888888888888889, 0.4222222222222222, 0.3888888888888889, 0.35555555555555557, 0.5] 5\n",
            "CNN alone:           \n",
            "test loss 0.1371750682592392\n",
            "top- 10 0.6630434782608695\n",
            "macro AUC 0.4148293365204529\n",
            "micro AUC 0.7152240206427398\n",
            "macro F1 0.0\n",
            "micro F1 0.0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "KSI+CNN:           \n",
            "test loss 0.13962231824795404\n",
            "top- 10 0.6413043478260869\n",
            "macro AUC 0.54708876925594\n",
            "micro AUC 0.7242943154273204\n",
            "macro F1 0.01020408163265306\n",
            "micro F1 0.030303030303030304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KSI_LSTM**"
      ],
      "metadata": {
        "id": "6tmJbkgTayqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "\n",
        "##########################################################\n",
        "\n",
        "label_to_ix=np.load('label_to_ix.npy',allow_pickle=True).item()\n",
        "ix_to_label=np.load('ix_to_label.npy',allow_pickle=True)\n",
        "training_data=np.load('training_data.npy',allow_pickle=True)\n",
        "test_data=np.load('test_data.npy',allow_pickle=True)\n",
        "val_data=np.load('val_data.npy',allow_pickle=True)\n",
        "word_to_ix=np.load('word_to_ix.npy',allow_pickle=True).item()\n",
        "ix_to_word=np.load('ix_to_word.npy',allow_pickle=True)\n",
        "newwikivec=np.load('newwikivec.npy',allow_pickle=True)\n",
        "wikivoc=np.load('wikivoc.npy',allow_pickle=True).item()\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "\n",
        "batchsize=8\n",
        "\n",
        "\n",
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    new_data=[]\n",
        "    for i, note, j in data:\n",
        "        templabel=[0.0]*len(label_to_ix)\n",
        "        for jj in j:\n",
        "            if jj in wikivoc:\n",
        "                templabel[label_to_ix[jj]]=1.0\n",
        "        templabel=np.array(templabel,dtype=float)\n",
        "        new_data.append((i, note, templabel))\n",
        "    new_data=np.array(new_data)\n",
        "    \n",
        "    lenlist=[]\n",
        "    for i in new_data:\n",
        "        lenlist.append(len(i[0]))\n",
        "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
        "    new_data=new_data[sortlen]\n",
        "    \n",
        "    batch_data=[]\n",
        "    \n",
        "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
        "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
        "        mybsize= len(thisblock)\n",
        "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
        "        main_matrix = np.zeros((mybsize, numword), dtype= np.int)\n",
        "        for i in range(main_matrix.shape[0]):\n",
        "            for j in range(main_matrix.shape[1]):\n",
        "                try:\n",
        "                    if thisblock[i][0][j] in word_to_ix:\n",
        "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
        "                    \n",
        "                except IndexError:\n",
        "                    pass       # because initialze with 0, so you pad with 0\n",
        "    \n",
        "        xxx2=[]\n",
        "        yyy=[]\n",
        "        for ii in thisblock:\n",
        "            xxx2.append(ii[1])\n",
        "            yyy.append(ii[2])\n",
        "        \n",
        "        xxx2=np.array(xxx2)\n",
        "        yyy=np.array(yyy)\n",
        "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
        "    return batch_data\n",
        "batchtraining_data=preprocessing(training_data)\n",
        "batchtest_data=preprocessing(test_data)\n",
        "batchval_data=preprocessing(val_data)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize=100\n",
        "hidden_dim=200\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(Embeddingsize, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.hidden = self.init_hidden()\n",
        "        \n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1,bias=False)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize,bias=False)\n",
        "        \n",
        "        self.softmax = nn.Softmax()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.embed_drop = nn.Dropout(p=0.2)\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return (autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim).cuda()),\n",
        "                autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim)).cuda())\n",
        "\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "      \n",
        "        thisembeddings=self.word_embeddings(vec1).transpose(0,1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "       \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            thisembeddings, self.hidden)\n",
        "        \n",
        "        lstm_out=lstm_out.transpose(0,2).transpose(0,1)\n",
        "        \n",
        "        output1=nn.MaxPool1d(lstm_out.size()[2])(lstm_out).view(batchsize,-1)\n",
        "        \n",
        "        vec2 = self.hidden2tag(output1)\n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(vec2.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(vec2)\n",
        "        \n",
        "        \n",
        "        return tag_scores\n",
        "\n",
        "######################################################################\n",
        "# Train the model:\n",
        "\n",
        "topk=10\n",
        "\n",
        "def trainmodel(model, sim):\n",
        "    print ('start_training')\n",
        "    modelsaved=[]\n",
        "    modelperform=[]\n",
        "    topk=10\n",
        "    \n",
        "    \n",
        "    bestresults=-1\n",
        "    bestiter=-1\n",
        "    for epoch in range(5000):  \n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        lossestrain = []\n",
        "        recall=[]\n",
        "        for mysentence in batchtraining_data:\n",
        "            model.zero_grad()\n",
        "            model.hidden = model.init_hidden()\n",
        "            targets = mysentence[2].cuda()\n",
        "            tag_scores = model(mysentence[0].cuda(),mysentence[1].cuda(),wikivec.cuda(),sim)\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossestrain.append(loss.data.mean())\n",
        "        print (epoch)\n",
        "        \n",
        "        modelsaved.append(copy.deepcopy(model.state_dict()))\n",
        "        print (\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
        "        model.eval()\n",
        "    \n",
        "        recall=[]\n",
        "        for inputs in batchval_data:\n",
        "            model.hidden = model.init_hidden()\n",
        "            targets = inputs[2].cuda()\n",
        "            tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "    \n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            \n",
        "            targets=targets.data.cpu().numpy()\n",
        "            tag_scores= tag_scores.data.cpu().numpy()\n",
        "            \n",
        "            \n",
        "            for iii in range(0,len(tag_scores)):\n",
        "                temp={}\n",
        "                for iiii in range(0,len(tag_scores[iii])):\n",
        "                    temp[iiii]=tag_scores[iii][iiii]\n",
        "                temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "                thistop=int(np.sum(targets[iii]))\n",
        "                hit=0.0\n",
        "                for ii in temp1[0:max(thistop,topk)]:\n",
        "                    if targets[iii][ii[0]]==1.0:\n",
        "                        hit=hit+1\n",
        "                if thistop!=0:\n",
        "                    recall.append(hit/thistop)\n",
        "            \n",
        "        print ('validation top-',topk, np.mean(recall))\n",
        "        \n",
        "        \n",
        "        \n",
        "        modelperform.append(np.mean(recall))\n",
        "        if modelperform[-1]>bestresults:\n",
        "            bestresults=modelperform[-1]\n",
        "            bestiter=len(modelperform)-1\n",
        "        \n",
        "        if (len(modelperform)-bestiter)>5:\n",
        "            print (modelperform,bestiter)\n",
        "            return modelsaved[bestiter]\n",
        "    \n",
        "model = LSTM(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "basemodel= trainmodel(model, 0)\n",
        "torch.save(basemodel, 'LSTM_model')\n",
        "\n",
        "model = LSTM(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "model.load_state_dict(basemodel)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "KSImodel= trainmodel(model, 1)\n",
        "torch.save(KSImodel, 'KSI_LSTM_model')\n",
        "\n",
        "def testmodel(modelstate, sim):\n",
        "    model = LSTM(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "    model.cuda()\n",
        "    model.load_state_dict(modelstate)\n",
        "    loss_function = nn.BCELoss()\n",
        "    model.eval()\n",
        "    recall=[]\n",
        "    lossestest = []\n",
        "    \n",
        "    y_true=[]\n",
        "    y_scores=[]\n",
        "    \n",
        "    \n",
        "    for inputs in batchtest_data:\n",
        "        model.hidden = model.init_hidden()\n",
        "        targets = inputs[2].cuda()\n",
        "        \n",
        "        tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        \n",
        "        targets=targets.data.cpu().numpy()\n",
        "        tag_scores= tag_scores.data.cpu().numpy()\n",
        "        \n",
        "        \n",
        "        lossestest.append(loss.data.mean())\n",
        "        y_true.append(targets)\n",
        "        y_scores.append(tag_scores)\n",
        "        \n",
        "        for iii in range(0,len(tag_scores)):\n",
        "            temp={}\n",
        "            for iiii in range(0,len(tag_scores[iii])):\n",
        "                temp[iiii]=tag_scores[iii][iiii]\n",
        "            temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "            thistop=int(np.sum(targets[iii]))\n",
        "            hit=0.0\n",
        "            \n",
        "            for ii in temp1[0:max(thistop,topk)]:\n",
        "                if targets[iii][ii[0]]==1.0:\n",
        "                    hit=hit+1\n",
        "            if thistop!=0:\n",
        "                recall.append(hit/thistop)\n",
        "    y_true=np.concatenate(y_true,axis=0)\n",
        "    y_scores=np.concatenate(y_scores,axis=0)\n",
        "    y_true=y_true.T\n",
        "    y_scores=y_scores.T\n",
        "    temptrue=[]\n",
        "    tempscores=[]\n",
        "    for  col in range(0,len(y_true)):\n",
        "        if np.sum(y_true[col])!=0:\n",
        "            temptrue.append(y_true[col])\n",
        "            tempscores.append(y_scores[col])\n",
        "    temptrue=np.array(temptrue)\n",
        "    tempscores=np.array(tempscores)\n",
        "    y_true=temptrue.T\n",
        "    y_scores=tempscores.T\n",
        "    y_pred=(y_scores>0.5).astype(np.int)\n",
        "    lossarray = []\n",
        "    for i in lossestest:\n",
        "        lossarray.append(i.item())\n",
        "        \n",
        "    print('test loss', np.mean(lossarray))\n",
        "    print ('top-',topk, np.mean(recall))\n",
        "    print ('macro AUC', roc_auc_score(y_true, y_scores,average='macro'))\n",
        "    print ('micro AUC', roc_auc_score(y_true, y_scores,average='micro'))\n",
        "    print ('macro F1', f1_score(y_true, y_pred, average='macro')  )\n",
        "    print ('micro F1', f1_score(y_true, y_pred, average='micro')  )\n",
        "\n",
        "print ('LSTM alone:           ')\n",
        "testmodel(basemodel, 0)\n",
        "print ('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
        "print ('KSI+LSTM:           ')\n",
        "testmodel(KSImodel, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KcxvDxdI3s9",
        "outputId": "6086af77-d948-45b9-e95a-161e2314cd07"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-fa94dd879dd3>:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  new_data=np.array(new_data)\n",
            "<ipython-input-70-fa94dd879dd3>:56: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  main_matrix = np.zeros((mybsize, numword), dtype= np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_training\n",
            "0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.4888888888888889\n",
            "1\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.43333333333333335\n",
            "2\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "3\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "4\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "5\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "6\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "7\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "[0.4888888888888889, 0.43333333333333335, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556] 2\n",
            "start_training\n",
            "0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "1\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "2\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "3\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "4\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "5\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5888888888888889\n",
            "6\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5888888888888889\n",
            "[0.5555555555555556, 0.611111111111111, 0.5555555555555556, 0.611111111111111, 0.611111111111111, 0.5888888888888889, 0.5888888888888889] 1\n",
            "LSTM alone:           \n",
            "test loss 0.138930914302667\n",
            "top- 10 0.6543478260869565\n",
            "macro AUC 0.3925631123459254\n",
            "micro AUC 0.7118096280657857\n",
            "macro F1 0.0\n",
            "micro F1 0.0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "KSI+LSTM:           \n",
            "test loss 0.13682982325553894\n",
            "top- 10 0.6239130434782608\n",
            "macro AUC 0.5700768076513827\n",
            "micro AUC 0.7324784319858211\n",
            "macro F1 0.0\n",
            "micro F1 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-fa94dd879dd3>:292: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_pred=(y_scores>0.5).astype(np.int)\n",
            "<ipython-input-70-fa94dd879dd3>:292: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_pred=(y_scores>0.5).astype(np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KSI_LSTMatt**"
      ],
      "metadata": {
        "id": "KNJo7SAaa8kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "\n",
        "##########################################################\n",
        "\n",
        "label_to_ix=np.load('label_to_ix.npy',allow_pickle=True).item()\n",
        "ix_to_label=np.load('ix_to_label.npy',allow_pickle=True)\n",
        "training_data=np.load('training_data.npy',allow_pickle=True)\n",
        "test_data=np.load('test_data.npy',allow_pickle=True)\n",
        "val_data=np.load('val_data.npy',allow_pickle=True)\n",
        "word_to_ix=np.load('word_to_ix.npy',allow_pickle=True).item()\n",
        "ix_to_word=np.load('ix_to_word.npy',allow_pickle=True)\n",
        "newwikivec=np.load('newwikivec.npy',allow_pickle=True)\n",
        "wikivoc=np.load('wikivoc.npy',allow_pickle=True).item()\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "batchsize=8\n",
        "\n",
        "\n",
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    new_data=[]\n",
        "    for i, note, j in data:\n",
        "        templabel=[0.0]*len(label_to_ix)\n",
        "        for jj in j:\n",
        "            if jj in wikivoc:\n",
        "                templabel[label_to_ix[jj]]=1.0\n",
        "        templabel=np.array(templabel,dtype=float)\n",
        "        new_data.append((i, note, templabel))\n",
        "    new_data=np.array(new_data)\n",
        "    \n",
        "    lenlist=[]\n",
        "    for i in new_data:\n",
        "        lenlist.append(len(i[0]))\n",
        "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
        "    new_data=new_data[sortlen]\n",
        "    \n",
        "    batch_data=[]\n",
        "    \n",
        "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
        "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
        "        mybsize= len(thisblock)\n",
        "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
        "        main_matrix = np.zeros((mybsize, numword), dtype= np.int)\n",
        "        for i in range(main_matrix.shape[0]):\n",
        "            for j in range(main_matrix.shape[1]):\n",
        "                try:\n",
        "                    if thisblock[i][0][j] in word_to_ix:\n",
        "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
        "                    \n",
        "                except IndexError:\n",
        "                    pass       # because initialze with 0, so you pad with 0\n",
        "    \n",
        "        xxx2=[]\n",
        "        yyy=[]\n",
        "        for ii in thisblock:\n",
        "            xxx2.append(ii[1])\n",
        "            yyy.append(ii[2])\n",
        "        \n",
        "        xxx2=np.array(xxx2)\n",
        "        yyy=np.array(yyy)\n",
        "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
        "    return batch_data\n",
        "batchtraining_data=preprocessing(training_data)\n",
        "batchtest_data=preprocessing(test_data)\n",
        "batchval_data=preprocessing(val_data)\n",
        "\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize=100\n",
        "hidden_dim=200\n",
        "class LSTMattn(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(LSTMattn, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(Embeddingsize, hidden_dim)\n",
        "        self.hidden = self.init_hidden()\n",
        "        \n",
        "        self.H=nn.Linear(hidden_dim, tagset_size )  \n",
        "        self.final = nn.Linear(hidden_dim, tagset_size)\n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1,bias=False)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize,bias=False)\n",
        "        \n",
        "        self.softmax = nn.Softmax()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.embed_drop = nn.Dropout(p=0.2)\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return (autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim).cuda()),\n",
        "                autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim)).cuda())\n",
        "\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "        \n",
        "        \n",
        "        thisembeddings=self.word_embeddings(vec1).transpose(0,1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "        \n",
        "        \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            thisembeddings, self.hidden)\n",
        "        \n",
        "        \n",
        "        \n",
        "        lstm_out=lstm_out.transpose(0,1)\n",
        "\n",
        "        alpha=self.H.weight.matmul(lstm_out.transpose(1,2))\n",
        "        alpha=F.softmax(alpha, dim=2)\n",
        "        \n",
        "        m=alpha.matmul(lstm_out)\n",
        "        \n",
        "        myfinal=self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "        \n",
        "        \n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(myfinal.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(myfinal)\n",
        "        \n",
        "        \n",
        "        return tag_scores\n",
        "\n",
        "######################################################################\n",
        "# Train the model:\n",
        "\n",
        "topk=10\n",
        "\n",
        "def trainmodel(model, sim):\n",
        "    print ('start_training')\n",
        "    modelsaved=[]\n",
        "    modelperform=[]\n",
        "    topk=10\n",
        "    \n",
        "    \n",
        "    bestresults=-1\n",
        "    bestiter=-1\n",
        "    for epoch in range(5000):  \n",
        "       \n",
        "        model.train()\n",
        "        \n",
        "        lossestrain = []\n",
        "        recall=[]\n",
        "        for mysentence in batchtraining_data:\n",
        "            model.zero_grad()\n",
        "            model.hidden = model.init_hidden()\n",
        "            targets = mysentence[2].cuda()\n",
        "            tag_scores = model(mysentence[0].cuda(),mysentence[1].cuda(),wikivec.cuda(),sim)\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossestrain.append(loss.data.mean())\n",
        "        print (epoch)\n",
        "        modelsaved.append(copy.deepcopy(model.state_dict()))\n",
        "        print (\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
        "        model.eval()\n",
        "    \n",
        "        recall=[]\n",
        "        for inputs in batchval_data:\n",
        "            model.hidden = model.init_hidden()\n",
        "            targets = inputs[2].cuda()\n",
        "            tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "    \n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            \n",
        "            targets=targets.data.cpu().numpy()\n",
        "            tag_scores= tag_scores.data.cpu().numpy()\n",
        "            \n",
        "            \n",
        "            for iii in range(0,len(tag_scores)):\n",
        "                temp={}\n",
        "                for iiii in range(0,len(tag_scores[iii])):\n",
        "                    temp[iiii]=tag_scores[iii][iiii]\n",
        "                temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "                thistop=int(np.sum(targets[iii]))\n",
        "                hit=0.0\n",
        "                for ii in temp1[0:max(thistop,topk)]:\n",
        "                    if targets[iii][ii[0]]==1.0:\n",
        "                        hit=hit+1\n",
        "                if thistop!=0:\n",
        "                    recall.append(hit/thistop)\n",
        "            \n",
        "        print ('validation top-',topk, np.mean(recall))\n",
        "        \n",
        "        \n",
        "        \n",
        "        modelperform.append(np.mean(recall))\n",
        "        if modelperform[-1]>bestresults:\n",
        "            bestresults=modelperform[-1]\n",
        "            bestiter=len(modelperform)-1\n",
        "        \n",
        "        if (len(modelperform)-bestiter)>5:\n",
        "            print (modelperform,bestiter)\n",
        "            return modelsaved[bestiter]\n",
        "    \n",
        "model = LSTMattn(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "basemodel= trainmodel(model, 0)\n",
        "torch.save(basemodel, 'LSTMattn_model')\n",
        "\n",
        "model = LSTMattn(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "model.load_state_dict(basemodel)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "KSImodel= trainmodel(model, 1)\n",
        "torch.save(KSImodel, 'KSI_LSTMattn_model')\n",
        "\n",
        "def testmodel(modelstate, sim):\n",
        "    model = LSTMattn(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "    model.cuda()\n",
        "    model.load_state_dict(modelstate)\n",
        "    loss_function = nn.BCELoss()\n",
        "    model.eval()\n",
        "    recall=[]\n",
        "    lossestest = []\n",
        "    \n",
        "    y_true=[]\n",
        "    y_scores=[]\n",
        "    \n",
        "    \n",
        "    for inputs in batchtest_data:\n",
        "        model.hidden = model.init_hidden()\n",
        "        targets = inputs[2].cuda()\n",
        "        \n",
        "        tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        \n",
        "        targets=targets.data.cpu().numpy()\n",
        "        tag_scores= tag_scores.data.cpu().numpy()\n",
        "        \n",
        "        \n",
        "        lossestest.append(loss.data.mean())\n",
        "        y_true.append(targets)\n",
        "        y_scores.append(tag_scores)\n",
        "        \n",
        "        for iii in range(0,len(tag_scores)):\n",
        "            temp={}\n",
        "            for iiii in range(0,len(tag_scores[iii])):\n",
        "                temp[iiii]=tag_scores[iii][iiii]\n",
        "            temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "            thistop=int(np.sum(targets[iii]))\n",
        "            hit=0.0\n",
        "            \n",
        "            for ii in temp1[0:max(thistop,topk)]:\n",
        "                if targets[iii][ii[0]]==1.0:\n",
        "                    hit=hit+1\n",
        "            if thistop!=0:\n",
        "                recall.append(hit/thistop)\n",
        "    y_true=np.concatenate(y_true,axis=0)\n",
        "    y_scores=np.concatenate(y_scores,axis=0)\n",
        "    y_true=y_true.T\n",
        "    y_scores=y_scores.T\n",
        "    temptrue=[]\n",
        "    tempscores=[]\n",
        "    for  col in range(0,len(y_true)):\n",
        "        if np.sum(y_true[col])!=0:\n",
        "            temptrue.append(y_true[col])\n",
        "            tempscores.append(y_scores[col])\n",
        "    temptrue=np.array(temptrue)\n",
        "    tempscores=np.array(tempscores)\n",
        "    y_true=temptrue.T\n",
        "    y_scores=tempscores.T\n",
        "    y_pred=(y_scores>0.5).astype(np.int)\n",
        "    lossarray = []\n",
        "    for i in lossestest:\n",
        "        lossarray.append(i.item())\n",
        "        \n",
        "    print('test loss', np.mean(lossarray))\n",
        "    print ('top-',topk, np.mean(recall))\n",
        "    print ('macro AUC', roc_auc_score(y_true, y_scores,average='macro'))\n",
        "    print ('micro AUC', roc_auc_score(y_true, y_scores,average='micro'))\n",
        "    print ('macro F1', f1_score(y_true, y_pred, average='macro')  )\n",
        "    print ('micro F1', f1_score(y_true, y_pred, average='micro')  )\n",
        "\n",
        "print ('LSTMattn alone:           ')\n",
        "testmodel(basemodel, 0)\n",
        "print ('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
        "print ('KSI+LSTMattn:           ')\n",
        "testmodel(KSImodel, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGVkAv8wJY4C",
        "outputId": "99fd4309-6237-4cdb-c742-c4f77e86c04e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-7b7b527c1884>:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  new_data=np.array(new_data)\n",
            "<ipython-input-71-7b7b527c1884>:56: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  main_matrix = np.zeros((mybsize, numword), dtype= np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_training\n",
            "0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.17777777777777778\n",
            "1\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5\n",
            "2\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "3\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "4\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "5\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "6\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "7\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "[0.17777777777777778, 0.5, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556] 2\n",
            "start_training\n",
            "0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5555555555555556\n",
            "1\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "2\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "3\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "4\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "5\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5777777777777777\n",
            "6\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.611111111111111\n",
            "[0.5555555555555556, 0.611111111111111, 0.611111111111111, 0.611111111111111, 0.611111111111111, 0.5777777777777777, 0.611111111111111] 1\n",
            "LSTMattn alone:           \n",
            "test loss 0.13454462587833405\n",
            "top- 10 0.6630434782608695\n",
            "macro AUC 0.5612608675210834\n",
            "micro AUC 0.722860791826309\n",
            "macro F1 0.0\n",
            "micro F1 0.0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "KSI+LSTMattn:           \n",
            "test loss 0.13308663417895636\n",
            "top- 10 0.6326086956521739\n",
            "macro AUC 0.6197534890224494\n",
            "micro AUC 0.7376130528839888\n",
            "macro F1 0.0\n",
            "micro F1 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-7b7b527c1884>:299: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_pred=(y_scores>0.5).astype(np.int)\n",
            "<ipython-input-71-7b7b527c1884>:299: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_pred=(y_scores>0.5).astype(np.int)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8733b8bb07f6651fc28f204aa0350299cd4d62533a7b4b0a7b820abdd2addc76"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}