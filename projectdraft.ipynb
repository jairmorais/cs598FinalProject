{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import string\n",
    "from stop_words import get_stop_words    # download stop words package from https://pypi.org/project/stop-words/\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "admidic=defaultdict(list)\n",
    "count=0\n",
    "\n",
    "\n",
    "with open('NOTEEVENTS.csv', 'r') as csvfile:\n",
    "     spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "     for row in spamreader:\n",
    "         dischargesummary = row[6].strip()\n",
    "        \n",
    "         if dischargesummary in \"Discharge Summary\":\n",
    "             admidic[row[1]].append(row[-1].replace('\\n',' ').translate(str.maketrans('','',string.punctuation)).lower())\n",
    "             count=count+1\n",
    "\n",
    "\n",
    "u=defaultdict(int)\n",
    "for i in admidic:\n",
    "    for jj in admidic[i]:\n",
    "        line=jj.strip('\\n').split()\n",
    "        for j in line:\n",
    "            u[j]=u[j]+1\n",
    "\n",
    "\n",
    "\n",
    "u2=defaultdict(int)\n",
    "for i in u:\n",
    "        if i.isdigit()==False:\n",
    "            if u[i]>10:\n",
    "                if i not in stop_words:\n",
    "                    u2[i]=u[i]\n",
    "                    \n",
    "u=[]   \n",
    "\n",
    "file1=codecs.open('DIAGNOSES_ICD.csv','r')\n",
    "ad2c=defaultdict(list)\n",
    "line=file1.readline()\n",
    "line=file1.readline()\n",
    "\n",
    "while line:\n",
    "    line=line.strip().split(',')\n",
    "\n",
    "    if line[4][1:-1]!='':\n",
    "        ad2c[line[2]].append(\"d_\"+line[4][1:-1])\n",
    "    \n",
    "    line=file1.readline()\n",
    "\n",
    "\n",
    "\n",
    "codeu=defaultdict(int)\n",
    "for i in ad2c:\n",
    "    for j in ad2c[i]:\n",
    "        codeu[j]=codeu[j]+1\n",
    "\n",
    "\n",
    "print(codeu)\n",
    "cthre=0\n",
    "fileo=codecs.open(\"combined_dataset\",'w')\n",
    "\n",
    "IDlist=np.load('IDlist.npy',encoding='bytes').astype(str)\n",
    "print(IDlist)\n",
    "for i in IDlist:\n",
    "    if ad2c[i]!=[]:\n",
    "        \n",
    "        fileo.write('start! '+i+'\\n')\n",
    "        fileo.write('codes: ')\n",
    "        tempc=[]\n",
    "        for code in ad2c[i]:\n",
    "            if codeu[code]>=cthre:\n",
    "                if code[0:5] not in tempc:\n",
    "                    tempc.append(code[0:5])\n",
    "       \n",
    "        for code in tempc:\n",
    "            fileo.write(code+\" \")\n",
    "        fileo.write('\\n')\n",
    "        fileo.write('notes:\\n')\n",
    "        for line in admidic[i]:    \n",
    "            thisline=line.strip('\\n').split() \n",
    "            print(thisline)\n",
    "            for j in thisline:\n",
    "                if u2[j]!=0:\n",
    "                    \n",
    "                    fileo.write(j+\" \")\n",
    "            fileo.write('\\n')\n",
    "        fileo.write('end!\\n')\n",
    "fileo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyhealth\n",
      "  Using cached pyhealth-1.1.3-py2.py3-none-any.whl (113 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 431 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.8.0\n",
      "  Downloading torch-2.0.0-cp39-none-macosx_10_9_x86_64.whl (139.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 139.8 MB 1.3 kB/s  eta 0:00:01  |▏                               | 829 kB 800 kB/s eta 0:02:54     |█                               | 4.5 MB 7.9 MB/s eta 0:00:18     |██▊                             | 11.7 MB 469 kB/s eta 0:04:33     |███▋                            | 16.0 MB 330 kB/s eta 0:06:15     |███▋                            | 16.0 MB 330 kB/s eta 0:06:15     |███▊                            | 16.3 MB 330 kB/s eta 0:06:14     |███▉                            | 16.9 MB 330 kB/s eta 0:06:12     |████                            | 17.9 MB 372 kB/s eta 0:05:28            | 19.8 MB 372 kB/s eta 0:05:23     |█████▊                          | 25.0 MB 389 kB/s eta 0:04:55     |██████                          | 26.1 MB 14.9 MB/s eta 0:00:08     |██████                          | 26.5 MB 14.9 MB/s eta 0:00:08     |██████▌                         | 28.5 MB 14.9 MB/s eta 0:00:08     |████████▎                       | 36.3 MB 400 kB/s eta 0:04:19▍                       | 36.5 MB 454 kB/s eta 0:03:48     |████████▌                       | 37.2 MB 454 kB/s eta 0:03:46     |████████▉                       | 38.4 MB 454 kB/s eta 0:03:44     |█████████▏                      | 40.0 MB 315 kB/s eta 0:05:17�████████▏                      | 40.1 MB 315 kB/s eta 0:05:16�████████▍                      | 41.2 MB 315 kB/s eta 0:05:13     |██████████▌                     | 46.0 MB 311 kB/s eta 0:05:0214.2 MB/s eta 0:00:07  | 49.8 MB 290 kB/s eta 0:05:10     |████████████                    | 52.2 MB 290 kB/s eta 0:05:02��█                   | 56.9 MB 362 kB/s eta 0:03:49     |█████████████▍                  | 58.5 MB 362 kB/s eta 0:03:45     |████████████████                | 69.9 MB 424 kB/s eta 0:02:45     |████████████████▍               | 71.5 MB 1.2 MB/s eta 0:00:57     |██████████████████              | 78.5 MB 12.7 MB/s eta 0:00:05            | 82.0 MB 479 kB/s eta 0:02:01     |███████████████████▋            | 85.6 MB 253 kB/s eta 0:03:3521     |█████████████████████           | 91.6 MB 630 kB/s eta 0:01:17     |█████████████████████▌          | 94.0 MB 630 kB/s eta 0:01:13     |██████████████████████▍         | 97.6 MB 415 kB/s eta 0:01:42     |██████████████████████▍         | 97.6 MB 415 kB/s eta 0:01:42        | 99.4 MB 415 kB/s eta 0:01:38     |███████████████████████▋        | 103.0 MB 22.7 MB/s eta 0:00:02     |██████████████████████████      | 113.6 MB 495 kB/s eta 0:00:53     |██████████████████████████      | 113.9 MB 495 kB/s eta 0:00:53     |██████████████████████████▏     | 114.3 MB 495 kB/s eta 0:00:52     |███████████████████████████▋    | 120.5 MB 8.4 MB/s eta 0:00:03     |█████████████████████████████   | 126.4 MB 2.2 MB/s eta 0:00:06.2 MB 383 kB/s eta 0:00:33�█▋  | 129.2 MB 383 kB/s eta 0:00:28     |█████████████████████████████▊  | 129.8 MB 333 kB/s eta 0:00:31�██  | 130.6 MB 333 kB/s eta 0:00:28████████████▏ | 131.8 MB 333 kB/s eta 0:00:25     |███████████████████████████████▏| 136.3 MB 332 kB/s eta 0:00:11��████████████▎| 136.5 MB 332 kB/s eta 0:00:10     |███████████████████████████████▍| 137.3 MB 1.5 MB/s eta 0:00:02�███████████▋| 138.1 MB 1.5 MB/s eta 0:00:02��████████████▊| 138.8 MB 1.5 MB/s eta 0:00:01     |███████████████████████████████▉| 139.3 MB 16.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.6.3\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting rdkit>=2022.03.4\n",
      "  Downloading rdkit-2022.9.5-cp39-cp39-macosx_10_9_x86_64.whl (24.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.8 MB 33 kB/s  eta 0:00:01    |██▌                             | 1.9 MB 1.7 MB/s eta 0:00:14                | 4.0 MB 2.0 MB/s eta 0:00:11  |████████                        | 6.1 MB 476 kB/s eta 0:00:40s eta 0:00:39     |██████████▋                     | 8.2 MB 345 kB/s eta 0:00:49     |███████████▊                    | 9.1 MB 345 kB/s eta 0:00:46�████████▊                  | 10.6 MB 503 kB/s eta 0:00:29     |██████████████▋                 | 11.3 MB 503 kB/s eta 0:00:27  | 12.5 MB 510 kB/s eta 0:00:25███▊              | 13.8 MB 308 kB/s eta 0:00:36     |██████████████████▏             | 14.1 MB 308 kB/s eta 0:00:35            | 14.6 MB 308 kB/s eta 0:00:34███████████▏            | 14.8 MB 308 kB/s eta 0:00:33�███████████▎       | 18.9 MB 466 kB/s eta 0:00:13     |██████████████████████████▌     | 20.5 MB 611 kB/s eta 0:00:08 |███████████████████████████▎    | 21.1 MB 611 kB/s eta 0:00:06�██████████▊| 24.6 MB 436 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.3.2 in /Users/jfilho/Library/Python/3.9/lib/python/site-packages (from pyhealth) (2.0.0)\n",
      "Collecting scikit-learn>=0.24.2\n",
      "  Downloading scikit_learn-1.2.2-cp39-cp39-macosx_10_9_x86_64.whl (9.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1 MB 43 kB/s  eta 0:00:01[K     |█                               | 286 kB 5.9 MB/s eta 0:00:02     |████                            | 1.1 MB 5.9 MB/s eta 0:00:02 440 kB/s eta 0:00:14███████████████                 | 4.2 MB 232 kB/s eta 0:00:21         | 4.4 MB 232 kB/s eta 0:00:21               | 4.5 MB 232 kB/s eta 0:00:20�█████████▏            | 5.5 MB 232 kB/s eta 0:00:16     |████████████████████▌           | 5.8 MB 209 kB/s eta 0:00:16| 6.5 MB 209 kB/s eta 0:00:13�██████████████████████     | 7.7 MB 464 kB/s eta 0:00:04███████████████▋    | 7.9 MB 464 kB/s eta 0:00:03�██▌ | 8.7 MB 464 kB/s eta 0:00:01��███████████████▉ | 8.8 MB 464 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /Users/jfilho/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3.2->pyhealth) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jfilho/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3.2->pyhealth) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jfilho/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3.2->pyhealth) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/jfilho/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3.2->pyhealth) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.2->pyhealth) (1.15.0)\n",
      "Collecting Pillow\n",
      "  Downloading Pillow-9.5.0-cp39-cp39-macosx_10_10_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 554 kB/s eta 0:00:011��              | 1.9 MB 15.6 MB/s eta 0:00:01███████████████▋      | 2.7 MB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.1-cp39-cp39-macosx_10_9_x86_64.whl (35.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 35.2 MB 20 kB/s  eta 0:00:012   |▎                               | 245 kB 7.5 MB/s eta 0:00:05:54:49     |████                            | 4.4 MB 631 kB/s eta 0:00:49| 5.3 MB 631 kB/s eta 0:00:48                          | 6.4 MB 631 kB/s eta 0:00:46               | 9.6 MB 232 kB/s eta 0:01:50�███▏                      | 10.1 MB 232 kB/s eta 0:01:48�████████▉                      | 10.8 MB 4.5 MB/s eta 0:00:06�█████                      | 10.9 MB 4.5 MB/s eta 0:00:06�█                      | 11.1 MB 4.5 MB/s eta 0:00:06��████▎                     | 11.3 MB 4.5 MB/s eta 0:00:06ta 0:00:06  | 12.4 MB 4.5 MB/s eta 0:00:06��███▌                    | 12.6 MB 202 kB/s eta 0:01:52��█                   | 14.3 MB 202 kB/s eta 0:01:43      | 15.8 MB 307 kB/s eta 0:01:03     |█████████████████               | 18.7 MB 557 kB/s eta 0:00:3046��██▉           | 22.9 MB 274 kB/s eta 0:00:45██████        | 26.3 MB 465 kB/s eta 0:00:20�███        | 26.5 MB 465 kB/s eta 0:00:19[K     |████████████████████████▏       | 26.6 MB 465 kB/s eta 0:00:19.2 MB 465 kB/s eta 0:00:18��█████████████████▎      | 27.8 MB 465 kB/s eta 0:00:16███████▍      | 27.9 MB 465 kB/s eta 0:00:16kB/s eta 0:00:22     |████████████████████████████    | 30.9 MB 339 kB/s eta 0:00:13  | 33.0 MB 301 kB/s eta 0:00:08��████▊ | 33.8 MB 301 kB/s eta 0:00:05�████████████████████████▏| 34.2 MB 301 kB/s eta 0:00:04�██████████▊| 34.8 MB 301 kB/s eta 0:00:02 |███████████████████████████████▊| 34.9 MB 301 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/jfilho/Library/Python/3.9/lib/python/site-packages (from torch>=1.8.0->pyhealth) (4.5.0)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.11.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.2-cp39-cp39-macosx_10_9_x86_64.whl (13 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, MarkupSafe, threadpoolctl, sympy, scipy, Pillow, networkx, joblib, jinja2, filelock, tqdm, torch, scikit-learn, rdkit, pyhealth\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/Users/jfilho/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/Users/jfilho/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/Users/jfilho/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed MarkupSafe-2.1.2 Pillow-9.5.0 filelock-3.11.0 jinja2-3.1.2 joblib-1.2.0 mpmath-1.3.0 networkx-3.1 pyhealth-1.1.3 rdkit-2022.9.5 scikit-learn-1.2.2 scipy-1.10.1 sympy-1.11.1 threadpoolctl-3.1.0 torch-2.0.0 tqdm-4.65.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "while line:\n",
    "    line=line.strip('\\n')\n",
    "    line=line.split()\n",
    "    if line[0]=='codes:':\n",
    "        line=file3.readline()\n",
    "        line=line.strip('\\n')\n",
    "        line=line.split()\n",
    "        \n",
    "        if  line[0]=='notes:':\n",
    "            tempf=[]\n",
    "            line=file3.readline()\n",
    "        \n",
    "            while line!='end!\\n':\n",
    "                line=line.strip('\\n')\n",
    "                line=line.split()\n",
    "                for word in line:\n",
    "                    if word in a3:\n",
    "                        tempf.append(word)\n",
    "                \n",
    "                line=file3.readline()\n",
    "                \n",
    "            \n",
    "            notesdocuments.append(tempf)\n",
    "    line=file3.readline()\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "notesvocab={}\n",
    "for i in notesdocuments:\n",
    "    for j in i:\n",
    "        if j.lower() not in notesvocab:\n",
    "            notesvocab[j.lower()]=len(notesvocab)\n",
    "notedata=[]\n",
    "for i in notesdocuments:\n",
    "    temp=''\n",
    "    for j in i:\n",
    "        temp=temp+j+\" \"\n",
    "    notedata.append(temp)\n",
    "    \n",
    "print(notesdocuments)\n",
    "wikidata=[]\n",
    "for i in wikidocuments:\n",
    "    temp=''\n",
    "    for j in i:\n",
    "        temp=temp+j+\" \"\n",
    "    wikidata.append(temp)    \n",
    "##########################################################\n",
    "print(notedata)\n",
    "vect = CountVectorizer(min_df=1,vocabulary=notesvocab,binary=True)\n",
    "binaryn = vect.fit_transform(notedata)\n",
    "binaryn=binaryn.A\n",
    "binaryn=np.array(binaryn,dtype=float)\n",
    "\n",
    "vect2 = CountVectorizer(min_df=1,vocabulary=notesvocab,binary=True)\n",
    "binaryk = vect2.fit_transform(wikidata)\n",
    "binaryk=binaryk.A\n",
    "binaryk=np.array(binaryk,dtype=float)\n",
    "\n",
    "\n",
    "np.save('notevec',binaryn)\n",
    "np.save('wikivec',binaryk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing PATIENTS and ADMISSIONS: 100%|██████████| 49993/49993 [11:40<00:00, 71.34it/s] \n",
      "Parsing DIAGNOSES_ICD: 100%|██████████| 52354/52354 [02:31<00:00, 345.44it/s] \n",
      "Mapping codes: 100%|██████████| 49993/49993 [04:42<00:00, 176.66it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import codecs\n",
    "\n",
    "\n",
    "##########################################################\n",
    "wikivoc={}\n",
    "codewiki=defaultdict(list)\n",
    "\n",
    "file2=codecs.open(\"wikipedia_knowledge\",'r','utf-8')\n",
    "line=file2.readline()\n",
    "count=0\n",
    "while line:\n",
    "    if line[0:4]=='XXXd':\n",
    "        line=line.strip('\\n')\n",
    "        line=line.split()\n",
    "        for i in line:\n",
    "            if i[0:2]=='d_':\n",
    "                codewiki[i].append(count)\n",
    "                wikivoc[i]=1\n",
    "        count=count+1\n",
    "    line=file2.readline()\n",
    "\n",
    "################### four codes have two wikidocuments, correct them\n",
    "codewiki['d_072']=[214]\n",
    "codewiki['d_698']=[125]\n",
    "codewiki['d_305']=[250]\n",
    "codewiki['d_386']=[219]\n",
    "\n",
    "np.save('wikivoc',wikivoc)\n",
    "##################################################\n",
    "filec=codecs.open(\"combined_dataset\",'r','utf-8')\n",
    "\n",
    "line=filec.readline()\n",
    "\n",
    "feature=[]\n",
    "label=[]\n",
    "\n",
    "while line:\n",
    "    line=line.strip('\\n')\n",
    "    line=line.split()\n",
    "    \n",
    "    if line[0]=='codes:':\n",
    "        temp=line[1:]\n",
    "        label.append(temp)\n",
    "        line=filec.readline()\n",
    "        line=line.strip('\\n')\n",
    "        line=line.split()\n",
    "        if  line[0]=='notes:':\n",
    "            tempf=[]\n",
    "            line=filec.readline()\n",
    "           \n",
    "            while line!='end!\\n':\n",
    "                line=line.strip('\\n')\n",
    "                line=line.split()\n",
    "                tempf=tempf+line\n",
    "                line=filec.readline()\n",
    "            feature.append(tempf)\n",
    "    line=filec.readline()\n",
    "\n",
    "\n",
    "prevoc={}\n",
    "for i in label:\n",
    "    for j in i:\n",
    "        if j not in prevoc:\n",
    "            prevoc[j]=len(prevoc)\n",
    "\n",
    "##################################\n",
    "notevec=np.load('notevec.npy')\n",
    "wikivec=np.load('wikivec.npy')\n",
    "label_to_ix = {}\n",
    "ix_to_label={}\n",
    "\n",
    "\n",
    "for codes in label:\n",
    "    for code in codes:\n",
    "        if code not in label_to_ix:\n",
    "            label_to_ix[code]=len(label_to_ix)\n",
    "            ix_to_label[label_to_ix[code]]=code\n",
    "\n",
    "tempwikivec=[]\n",
    "\n",
    "for i in range(0,len(ix_to_label)):\n",
    "    if ix_to_label[i] in wikivoc:\n",
    "        temp=wikivec[codewiki[ix_to_label[i]][0]]\n",
    "        tempwikivec.append(temp)\n",
    "    else:\n",
    "        tempwikivec.append([0.0]*wikivec.shape[1])\n",
    "wikivec=np.array(tempwikivec)\n",
    "\n",
    "####################################\n",
    "\n",
    "data=[]\n",
    "for i in range(0,len(feature)):\n",
    "    data.append((feature[i], notevec[i], label[i]))\n",
    "    \n",
    "data=np.array(data)  \n",
    "\n",
    "label_to_ix = {}\n",
    "ix_to_label={}\n",
    "\n",
    "for doc, note, codes in data:\n",
    "    for code in codes:\n",
    "        if code not in label_to_ix:\n",
    "            if code in wikivoc:\n",
    "                label_to_ix[code]=len(label_to_ix)\n",
    "                ix_to_label[label_to_ix[code]]=code\n",
    "\n",
    "np.save('label_to_ix',label_to_ix)\n",
    "np.save('ix_to_label',ix_to_label)\n",
    "\n",
    "training_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "training_data, val_data = train_test_split(training_data, test_size=0.125, random_state=42)\n",
    "\n",
    "np.save('training_data',training_data)\n",
    "np.save('test_data',test_data)\n",
    "np.save('val_data',val_data)\n",
    "\n",
    "\n",
    "word_to_ix = {}\n",
    "ix_to_word={}\n",
    "ix_to_word[0]='OUT'\n",
    "\n",
    "\n",
    "for doc, note, codes in training_data:\n",
    "    for word in doc:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)+1\n",
    "            ix_to_word[word_to_ix[word]]=word  \n",
    "    \n",
    "np.save('word_to_ix',word_to_ix)\n",
    "np.save('ix_to_word',ix_to_word)\n",
    "\n",
    "newwikivec=[]\n",
    "for i in range(0,len(ix_to_label)):\n",
    "    newwikivec.append(wikivec[prevoc[ix_to_label[i]]])\n",
    "newwikivec=np.array(newwikivec)\n",
    "np.save('newwikivec',newwikivec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "torch.manual_seed(1)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "\n",
    "##########################################################\n",
    "\n",
    "label_to_ix=np.load('label_to_ix.npy').item()\n",
    "ix_to_label=np.load('ix_to_label.npy')\n",
    "training_data=np.load('training_data.npy')\n",
    "test_data=np.load('test_data.npy')\n",
    "val_data=np.load('val_data.npy')\n",
    "word_to_ix=np.load('word_to_ix.npy').item()\n",
    "ix_to_word=np.load('ix_to_word.npy')\n",
    "newwikivec=np.load('newwikivec.npy')\n",
    "wikivoc=np.load('wikivoc.npy').item()\n",
    "\n",
    "wikisize=newwikivec.shape[0]\n",
    "rvocsize=newwikivec.shape[1]\n",
    "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
    "\n",
    "\n",
    "batchsize=32\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(data):\n",
    "\n",
    "    new_data=[]\n",
    "    for i, note, j in data:\n",
    "        templabel=[0.0]*len(label_to_ix)\n",
    "        for jj in j:\n",
    "            if jj in wikivoc:\n",
    "                templabel[label_to_ix[jj]]=1.0\n",
    "        templabel=np.array(templabel,dtype=float)\n",
    "        new_data.append((i, note, templabel))\n",
    "    new_data=np.array(new_data)\n",
    "    \n",
    "    lenlist=[]\n",
    "    for i in new_data:\n",
    "        lenlist.append(len(i[0]))\n",
    "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
    "    new_data=new_data[sortlen]\n",
    "    \n",
    "    batch_data=[]\n",
    "    \n",
    "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
    "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
    "        mybsize= len(thisblock)\n",
    "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
    "        main_matrix = np.zeros((mybsize, numword), dtype= np.int)\n",
    "        for i in range(main_matrix.shape[0]):\n",
    "            for j in range(main_matrix.shape[1]):\n",
    "                try:\n",
    "                    if thisblock[i][0][j] in word_to_ix:\n",
    "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
    "                    \n",
    "                except IndexError:\n",
    "                    pass       # because initialze with 0, so you pad with 0\n",
    "    \n",
    "        xxx2=[]\n",
    "        yyy=[]\n",
    "        for ii in thisblock:\n",
    "            xxx2.append(ii[1])\n",
    "            yyy.append(ii[2])\n",
    "        \n",
    "        xxx2=np.array(xxx2)\n",
    "        yyy=np.array(yyy)\n",
    "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
    "    return batch_data\n",
    "batchtraining_data=preprocessing(training_data)\n",
    "batchtest_data=preprocessing(test_data)\n",
    "batchval_data=preprocessing(val_data)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Create the model:\n",
    "\n",
    "Embeddingsize=100\n",
    "hidden_dim=200\n",
    "class CAML(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
    "        super(CAML, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
    "        self.embed_drop = nn.Dropout(p=0.2)   \n",
    "        \n",
    "        \n",
    "        self.convs1 = nn.Conv1d(Embeddingsize,300,10,padding=5)\n",
    "        self.H=nn.Linear(300, tagset_size )   \n",
    "        self.final = nn.Linear(300, tagset_size)\n",
    "        \n",
    "        self.layer2 = nn.Linear(Embeddingsize, 1)\n",
    "        self.embedding=nn.Linear(rvocsize,Embeddingsize,bias=False)\n",
    "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, vec1, nvec, wiki, simlearning):\n",
    "        \n",
    "       \n",
    "        thisembeddings=self.word_embeddings(vec1)\n",
    "        thisembeddings = self.embed_drop(thisembeddings)\n",
    "        thisembeddings=thisembeddings.transpose(1,2)\n",
    "        \n",
    "        \n",
    "        thisembeddings=self.tanh(self.convs1(thisembeddings).transpose(1,2))  \n",
    "        \n",
    "        alpha=self.H.weight.matmul(thisembeddings.transpose(1,2))\n",
    "        alpha=F.softmax(alpha, dim=2)\n",
    "        \n",
    "        m=alpha.matmul(thisembeddings)\n",
    "       \n",
    "        myfinal=self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
    "        \n",
    "        if simlearning==1:\n",
    "            nvec=nvec.view(batchsize,1,-1)\n",
    "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
    "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
    "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
    "            new=wiki*nvec\n",
    "            new=self.embedding(new)\n",
    "            vattention=self.sigmoid(self.vattention(new))\n",
    "            new=new*vattention\n",
    "            vec3=self.layer2(new)\n",
    "            vec3=vec3.view(batchsize,-1)\n",
    "        \n",
    "       \n",
    "        if simlearning==1:\n",
    "            tag_scores = self.sigmoid(myfinal.detach()+vec3)\n",
    "        else:\n",
    "            tag_scores = self.sigmoid(myfinal)\n",
    "        \n",
    "        \n",
    "        return tag_scores\n",
    "\n",
    "\n",
    "\n",
    "topk=10\n",
    "\n",
    "def trainmodel(model, sim):\n",
    "    print ('start_training')\n",
    "    modelsaved=[]\n",
    "    modelperform=[]\n",
    "    topk=10\n",
    "    \n",
    "    \n",
    "    bestresults=-1\n",
    "    bestiter=-1\n",
    "    for epoch in range(5000):  \n",
    "        model.train()\n",
    "        \n",
    "        lossestrain = []\n",
    "        recall=[]\n",
    "        for mysentence in batchtraining_data:\n",
    "            model.zero_grad()\n",
    "            \n",
    "            targets = mysentence[2].cuda()\n",
    "            tag_scores = model(mysentence[0].cuda(),mysentence[1].cuda(),wikivec.cuda(),sim)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lossestrain.append(loss.data.mean())\n",
    "        print (epoch)\n",
    "        modelsaved.append(copy.deepcopy(model.state_dict()))\n",
    "        print (\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "        model.eval()\n",
    "    \n",
    "        recall=[]\n",
    "        for inputs in batchval_data:\n",
    "           \n",
    "            targets = inputs[2].cuda()\n",
    "            tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
    "    \n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            targets=targets.data.cpu().numpy()\n",
    "            tag_scores= tag_scores.data.cpu().numpy()\n",
    "            \n",
    "            \n",
    "            for iii in range(0,len(tag_scores)):\n",
    "                temp={}\n",
    "                for iiii in range(0,len(tag_scores[iii])):\n",
    "                    temp[iiii]=tag_scores[iii][iiii]\n",
    "                temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
    "                thistop=int(np.sum(targets[iii]))\n",
    "                hit=0.0\n",
    "                for ii in temp1[0:max(thistop,topk)]:\n",
    "                    if targets[iii][ii[0]]==1.0:\n",
    "                        hit=hit+1\n",
    "                if thistop!=0:\n",
    "                    recall.append(hit/thistop)\n",
    "            \n",
    "        print ('validation top-',topk, np.mean(recall))\n",
    "        \n",
    "        \n",
    "        \n",
    "        modelperform.append(np.mean(recall))\n",
    "        if modelperform[-1]>bestresults:\n",
    "            bestresults=modelperform[-1]\n",
    "            bestiter=len(modelperform)-1\n",
    "        \n",
    "        if (len(modelperform)-bestiter)>5:\n",
    "            print (modelperform,bestiter)\n",
    "            return modelsaved[bestiter]\n",
    "    \n",
    "model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
    "model.cuda()\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "basemodel= trainmodel(model, 0)\n",
    "torch.save(basemodel, 'CAML_model')\n",
    "\n",
    "model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
    "model.cuda()\n",
    "model.load_state_dict(basemodel)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "KSImodel= trainmodel(model, 1)\n",
    "torch.save(KSImodel, 'KSI_CAML_model')\n",
    "\n",
    "def testmodel(modelstate, sim):\n",
    "    model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
    "    model.cuda()\n",
    "    model.load_state_dict(modelstate)\n",
    "    loss_function = nn.BCELoss()\n",
    "    model.eval()\n",
    "    recall=[]\n",
    "    lossestest = []\n",
    "    \n",
    "    y_true=[]\n",
    "    y_scores=[]\n",
    "    \n",
    "    \n",
    "    for inputs in batchtest_data:\n",
    "       \n",
    "        targets = inputs[2].cuda()\n",
    "        \n",
    "        tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
    "\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        \n",
    "        targets=targets.data.cpu().numpy()\n",
    "        tag_scores= tag_scores.data.cpu().numpy()\n",
    "        \n",
    "        \n",
    "        lossestest.append(loss.data.mean())\n",
    "        y_true.append(targets)\n",
    "        y_scores.append(tag_scores)\n",
    "        \n",
    "        for iii in range(0,len(tag_scores)):\n",
    "            temp={}\n",
    "            for iiii in range(0,len(tag_scores[iii])):\n",
    "                temp[iiii]=tag_scores[iii][iiii]\n",
    "            temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
    "            thistop=int(np.sum(targets[iii]))\n",
    "            hit=0.0\n",
    "            \n",
    "            for ii in temp1[0:max(thistop,topk)]:\n",
    "                if targets[iii][ii[0]]==1.0:\n",
    "                    hit=hit+1\n",
    "            if thistop!=0:\n",
    "                recall.append(hit/thistop)\n",
    "    y_true=np.concatenate(y_true,axis=0)\n",
    "    y_scores=np.concatenate(y_scores,axis=0)\n",
    "    y_true=y_true.T\n",
    "    y_scores=y_scores.T\n",
    "    temptrue=[]\n",
    "    tempscores=[]\n",
    "    for  col in range(0,len(y_true)):\n",
    "        if np.sum(y_true[col])!=0:\n",
    "            temptrue.append(y_true[col])\n",
    "            tempscores.append(y_scores[col])\n",
    "    temptrue=np.array(temptrue)\n",
    "    tempscores=np.array(tempscores)\n",
    "    y_true=temptrue.T\n",
    "    y_scores=tempscores.T\n",
    "    y_pred=(y_scores>0.5).astype(np.int)\n",
    "    print ('test loss', np.mean(lossestest))\n",
    "    print ('top-',topk, np.mean(recall))\n",
    "    print ('macro AUC', roc_auc_score(y_true, y_scores,average='macro'))\n",
    "    print ('micro AUC', roc_auc_score(y_true, y_scores,average='micro'))\n",
    "    print ('macro F1', f1_score(y_true, y_pred, average='macro')  )\n",
    "    print ('micro F1', f1_score(y_true, y_pred, average='micro')  )\n",
    "\n",
    "print ('CAML alone:           ')\n",
    "testmodel(basemodel, 0)\n",
    "print ('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print ('KSI+CAML:           ')\n",
    "testmodel(KSImodel, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
